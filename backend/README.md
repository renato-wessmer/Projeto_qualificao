### **README.md COMPLETO** ğŸ“š

Crie `backend/README.md`:

```markdown
# ğŸ¤– SOS Libras - Backend

Backend do sistema SOS Libras para reconhecimento de gestos em LÃ­ngua Brasileira de Sinais (Libras) usando Deep Learning.

---

## ğŸ“‹ Ãndice

1. [VisÃ£o Geral](#-visÃ£o-geral)
2. [Requisitos](#-requisitos)
3. [InstalaÃ§Ã£o](#-instalaÃ§Ã£o)
4. [Pipeline Completo](#-pipeline-completo)
5. [Guia de GravaÃ§Ã£o](#-guia-de-gravaÃ§Ã£o)
6. [Processamento de VÃ­deos](#-processamento-de-vÃ­deos)
7. [Treinamento do Modelo](#-treinamento-do-modelo)
8. [API REST](#-api-rest)
9. [IntegraÃ§Ã£o com Frontend](#-integraÃ§Ã£o-com-frontend)
10. [Troubleshooting](#-troubleshooting)

---

## ğŸ¯ VisÃ£o Geral

Este backend Ã© responsÃ¡vel por:

- âœ… Processar vÃ­deos de gestos em Libras
- âœ… Extrair keypoints (poses) usando MediaPipe
- âœ… Treinar modelo LSTM para reconhecimento
- âœ… Fornecer API REST para reconhecimento em tempo real
- âœ… Reconhecer sequÃªncias de gestos (ex: CEP, telefone)

### Tecnologias

- **Python 3.12**
- **TensorFlow 2.18** - Deep Learning
- **MediaPipe 0.10** - ExtraÃ§Ã£o de keypoints
- **OpenCV 4.11** - Processamento de vÃ­deo
- **Flask 3.0** - API REST
- **Rembg 2.0** - RemoÃ§Ã£o de fundo

---

## ğŸ’» Requisitos

### Hardware

- **CPU**: Qualquer processador moderno
- **RAM**: MÃ­nimo 8GB (recomendado 16GB)
- **GPU**: Opcional (NVIDIA com CUDA para treinamento mais rÃ¡pido)
- **EspaÃ§o**: ~5GB para vÃ­deos e modelo

### Software

- Python 3.12
- pip
- Webcam ou cÃ¢mera (para gravaÃ§Ã£o)
- Chroma key verde (fundo verde)
- IluminaÃ§Ã£o adequada

---

## ğŸ“¦ InstalaÃ§Ã£o

### 1. Clonar repositÃ³rio

```bash
cd Projeto_qualificao/backend
```

### 2. Criar ambiente virtual

```bash
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# ou: venv\Scripts\activate  # Windows
```

### 3. Instalar dependÃªncias

```bash
pip install -r requirements.txt
```

Isso instalarÃ¡:
- TensorFlow, Keras
- MediaPipe, OpenCV
- Flask, Flask-CORS
- Rembg (remoÃ§Ã£o de fundo)
- Numpy, Pandas, Scikit-learn

### 4. Verificar instalaÃ§Ã£o

```bash
python -c "import tensorflow as tf; print(f'TensorFlow: {tf.__version__}')"
python -c "import mediapipe as mp; print('MediaPipe OK')"
```

---

## ğŸ”„ Pipeline Completo

```
1. GRAVAÃ‡ÃƒO
   ğŸ“¹ Gravar vÃ­deos com chroma key verde
   â†“
2. REMOVER FUNDO (Script 1)
   ğŸ¨ Substituir verde por branco
   â†“
3. EXTRAIR FRAMES (Script 2)
   ğŸï¸ Gerar imagens de cada vÃ­deo
   â†“
4. EXTRAIR KEYPOINTS (Script 3)
   ğŸ¦´ MediaPipe detecta poses/mÃ£os/face
   â†“
5. PREPARAR DATASET (Script 4)
   ğŸ“Š Organizar dados para treino
   â†“
6. TREINAR MODELO (Script 5)
   ğŸ¤– Treinar rede neural LSTM
   â†“
7. API REST
   ğŸŒ Servir modelo para frontend
```

---

## ğŸ¬ Guia de GravaÃ§Ã£o

### Setup

1. **Chroma Key Verde**
   - Esticar bem (sem rugas)
   - IluminaÃ§Ã£o uniforme
   - Evitar sombras

2. **CÃ¢mera**
   - TripÃ© fixo
   - 1080p mÃ­nimo
   - 30fps
   - Enquadramento: da cintura para cima

3. **Roupa**
   - Escura (preto, azul, vermelho)
   - **NUNCA verde!**

4. **IluminaÃ§Ã£o**
   - 2 luzes no fundo verde
   - 1 luz frontal na pessoa
   - Luz suave e difusa

### Protocolo de GravaÃ§Ã£o

Para cada gesto (0-9, SIM, NÃƒO), grave **15 vÃ­deos** com variaÃ§Ãµes:

#### VariaÃ§Ãµes de Velocidade (3 vÃ­deos)
- Gesto normal
- Gesto rÃ¡pido
- Gesto lento

#### VariaÃ§Ãµes de PosiÃ§Ã£o (5 vÃ­deos)
- MÃ£o centralizada
- MÃ£o Ã  direita
- MÃ£o Ã  esquerda
- MÃ£o mais alta
- MÃ£o mais baixa

#### VariaÃ§Ãµes de DistÃ¢ncia (2 vÃ­deos)
- MÃ£o perto da cÃ¢mera
- MÃ£o longe da cÃ¢mera

#### VariaÃ§Ãµes de Contexto (5 vÃ­deos)
- Com movimento de entrada
- Com movimento de saÃ­da
- Pausado (2 segundos)
- Corpo inclinado
- ExpressÃ£o neutra

### OrganizaÃ§Ã£o dos Arquivos

```
data/raw_videos/
â”œâ”€â”€ numero_0/
â”‚   â”œâ”€â”€ video_001.mp4
â”‚   â”œâ”€â”€ video_002.mp4
â”‚   â””â”€â”€ ... (15 vÃ­deos)
â”œâ”€â”€ numero_1/
â”œâ”€â”€ numero_2/
â”œâ”€â”€ numero_3/
â”œâ”€â”€ numero_4/
â”œâ”€â”€ numero_5/
â”œâ”€â”€ numero_6/
â”œâ”€â”€ numero_7/
â”œâ”€â”€ numero_8/
â”œâ”€â”€ numero_9/
â”œâ”€â”€ sim/
â””â”€â”€ nao/
```

**Total**: 12 gestos Ã— 15 vÃ­deos = **180 vÃ­deos**

### Nomenclatura

- `numero_0` a `numero_9` - NÃºmeros de 0 a 9
- `sim` - Gesto de "sim" em Libras
- `nao` - Gesto de "nÃ£o" em Libras

---

## ğŸ¨ Processamento de VÃ­deos

### Script 1: Remover Fundo Verde

```bash
python scripts/1_remove_background.py
```

**O que faz:**
- LÃª vÃ­deos de `data/raw_videos/`
- Remove fundo verde usando Rembg
- Coloca fundo branco
- Salva em `data/processed_videos/`

**Tempo estimado**: ~30-60 minutos (180 vÃ­deos)

---

### Script 2: Extrair Frames

```bash
python scripts/2_extract_frames.py
```

**O que faz:**
- LÃª vÃ­deos de `data/processed_videos/`
- Extrai atÃ© 30 frames por vÃ­deo
- Salva como PNG em `data/frames/`

**Tempo estimado**: ~10-15 minutos

---

### Script 3: Extrair Keypoints

```bash
python scripts/3_extract_keypoints.py
```

**O que faz:**
- LÃª frames de `data/frames/`
- Usa MediaPipe para detectar:
  - Pose (corpo): 33 pontos
  - MÃ£o esquerda: 21 pontos
  - MÃ£o direita: 21 pontos
  - Face: 70 pontos
- Salva JSON em `data/keypoints/`

**Tempo estimado**: ~20-30 minutos

**SaÃ­da**: `keypoints.json` para cada vÃ­deo

---

### Script 4: Preparar Dataset

```bash
python scripts/4_prepare_dataset.py
```

**O que faz:**
- LÃª todos os keypoints
- Padroniza sequÃªncias
- Divide em treino/validaÃ§Ã£o/teste (70%/15%/15%)
- Salva arrays NumPy em `data/dataset/`

**Arquivos gerados**:
- `X_train.npy`, `y_train.npy`
- `X_val.npy`, `y_val.npy`
- `X_test.npy`, `y_test.npy`
- `gesture_mapping.json` (mapa de classes)
- `dataset_info.json` (metadados)

**Tempo estimado**: ~2-5 minutos

---

## ğŸ¤– Treinamento do Modelo

### Script 5: Treinar Modelo

```bash
python scripts/5_train_model.py
```

**O que faz:**
- Carrega dataset preparado
- ConstrÃ³i rede LSTM:
  - 3 camadas LSTM (128â†’64â†’32)
  - 2 camadas Dense
  - Dropout para regularizaÃ§Ã£o
- Treina por atÃ© 100 Ã©pocas
- Usa Early Stopping e ReduceLROnPlateau
- Salva melhor modelo

**Tempo estimado**: 
- CPU: 30-60 minutos
- GPU: 10-15 minutos

**Arquivos gerados**:
- `models/trained/best_gesture_model.h5` â­ (melhor modelo)
- `models/trained/gesture_model_TIMESTAMP.h5` (modelo final)
- `models/trained/training_history_TIMESTAMP.png` (grÃ¡ficos)
- `models/trained/training_history_TIMESTAMP.json` (histÃ³rico)

### MÃ©tricas Esperadas

Com 180 vÃ­deos (15 por classe):

- **AcurÃ¡cia de treino**: 90-95%
- **AcurÃ¡cia de validaÃ§Ã£o**: 85-92%
- **AcurÃ¡cia de teste**: 80-90%

âš ï¸ Se acurÃ¡cia < 80%, considere:
- Gravar mais vÃ­deos
- Melhorar iluminaÃ§Ã£o
- Verificar qualidade dos gestos

---

## ğŸŒ API REST

### Iniciar API

```bash
python run_api.py
```

**SaÃ­da esperada**:
```
======================================================================
ğŸš€ SOS LIBRAS API
======================================================================
ğŸŒ Host: 0.0.0.0:5000
ğŸ“ Modelo: models/trained/best_gesture_model.h5
ğŸ› Debug: True
======================================================================
ğŸ¦´ Inicializando MediaPipe...
âœ… Mapeamentos carregados:
  Gestos: ['nao', 'numero_0', 'numero_1', ...]
  Classes: 12
ğŸ“¦ Carregando modelo de: models/trained/best_gesture_model.h5
âœ… Modelo carregado com sucesso!
ğŸš€ Inicializando API...
âœ… API pronta!
 * Running on http://0.0.0.0:5000
```

### Endpoints

#### 1. Health Check

```bash
GET /api/health
```

**Resposta**:
```json
{
  "status": "ok",
  "model_loaded": true,
  "mediapipe_ready": true
}
```

---

#### 2. Listar Gestos

```bash
GET /api/gestures
```

**Resposta**:
```json
{
  "gestures": ["numero_0", "numero_1", ..., "sim", "nao"],
  "num_classes": 12
}
```

---

#### 3. Reconhecer Gesto (Single)

```bash
POST /api/recognize
Content-Type: multipart/form-data

Body:
  video: <arquivo.mp4>
```

**Exemplo com cURL**:
```bash
curl -X POST http://localhost:5000/api/recognize \
  -F "video=@gesto_5.mp4"
```

**Resposta**:
```json
{
  "gesture": "numero_5",
  "confidence": 0.95,
  "num_frames": 28,
  "probabilities": {
    "numero_0": 0.01,
    "numero_1": 0.00,
    "numero_2": 0.00,
    "numero_3": 0.01,
    "numero_4": 0.02,
    "numero_5": 0.95,
    "numero_6": 0.00,
    "numero_7": 0.00,
    "numero_8": 0.00,
    "numero_9": 0.01,
    "sim": 0.00,
    "nao": 0.00
  }
}
```

---

#### 4. Reconhecer SequÃªncia (CEP, etc)

```bash
POST /api/recognize-sequence
Content-Type: multipart/form-data

Body:
  videos[]: <gesto1.mp4>
  videos[]: <gesto2.mp4>
  videos[]: <gesto3.mp4>
  ...
```

**Exemplo com cURL**:
```bash
curl -X POST http://localhost:5000/api/recognize-sequence \
  -F "videos=@gesto_9.mp4" \
  -F "videos=@gesto_0.mp4" \
  -F "videos=@gesto_0.mp4" \
  -F "videos=@gesto_1.mp4" \
  -F "videos=@gesto_0.mp4" \
  -F "videos=@gesto_0.mp4" \
  -F "videos=@gesto_0.mp4" \
  -F "videos=@gesto_0.mp4"
```

**Resposta**:
```json
{
  "sequence": ["numero_9", "numero_0", "numero_0", "numero_1", "numero_0", "numero_0", "numero_0", "numero_0"],
  "formatted": "90010-000",
  "count": 8,
  "details": [
    {
      "gesture": "numero_9",
      "confidence": 0.98,
      "index": 0
    },
    ...
  ]
}
```

---

## ğŸ”— IntegraÃ§Ã£o com Frontend

### JavaScript/React

```javascript
// services/gestureAPI.js
import axios from 'axios';

const API_URL = 'http://localhost:5000/api';

export const recognizeGesture = async (videoBlob) => {
  const formData = new FormData();
  formData.append('video', videoBlob, 'gesture.mp4');
  
  const response = await axios.post(
    `${API_URL}/recognize`,
    formData,
    {
      headers: { 'Content-Type': 'multipart/form-data' }
    }
  );
  
  return response.data;
};

export const recognizeSequence = async (videoBlobs) => {
  const formData = new FormData();
  
  videoBlobs.forEach((blob, index) => {
    formData.append('videos', blob, `gesture_${index}.mp4`);
  });
  
  const response = await axios.post(
    `${API_URL}/recognize-sequence`,
    formData,
    {
      headers: { 'Content-Type': 'multipart/form-data' }
    }
  );
  
  return response.data;
};
```

### Exemplo de Uso no React

```jsx
import { useState } from 'react';
import { recognizeGesture } from './services/gestureAPI';

function GestureCapture() {
  const [result, setResult] = useState(null);
  const [loading, setLoading] = useState(false);
  
  const handleVideoCapture = async (videoBlob) => {
    setLoading(true);
    
    try {
      const data = await recognizeGesture(videoBlob);
      setResult(data);
      
      // Preencher campo automaticamente
      if (data.confidence > 0.8) {
        // document.getElementById('cep').value += data.gesture;
      }
    } catch (error) {
      console.error('Erro:', error);
    } finally {
      setLoading(false);
    }
  };
  
  return (
    <div>
      {/* Seu componente de captura de webcam */}
      {loading && <p>Reconhecendo gesto...</p>}
      {result && (
        <div>
          <p>Gesto: {result.gesture}</p>
          <p>ConfianÃ§a: {(result.confidence * 100).toFixed(1)}%</p>
        </div>
      )}
    </div>
  );
}
```

---

## âš ï¸ Troubleshooting

### Problema: `ModuleNotFoundError: No module named 'tensorflow'`

**SoluÃ§Ã£o**:
```bash
source venv/bin/activate  # Ativar ambiente virtual
pip install -r requirements.txt
```

---

### Problema: `FileNotFoundError: best_gesture_model.h5`

**Causa**: Modelo nÃ£o foi treinado ainda

**SoluÃ§Ã£o**:
```bash
# Rodar pipeline completo
python scripts/1_remove_background.py
python scripts/2_extract_frames.py
python scripts/3_extract_keypoints.py
python scripts/4_prepare_dataset.py
python scripts/5_train_model.py
```

---

### Problema: AcurÃ¡cia baixa (< 70%)

**Causas possÃ­veis**:
- Poucos vÃ­deos de treino
- MÃ¡ qualidade de gravaÃ§Ã£o
- IluminaÃ§Ã£o ruim
- Gestos inconsistentes

**SoluÃ§Ãµes**:
1. Gravar mais vÃ­deos (20-30 por gesto)
2. Melhorar setup de gravaÃ§Ã£o
3. Verificar se gestos estÃ£o corretos
4. Aumentar data augmentation

---

### Problema: API retorna erro 500

**Debug**:
```bash
# Verificar logs do terminal onde API estÃ¡ rodando
# Testar endpoint de health
curl http://localhost:5000/api/health

# Verificar se modelo existe
ls models/trained/best_gesture_model.h5
```

---

### Problema: MediaPipe nÃ£o detecta mÃ£os

**Causas**:
- IluminaÃ§Ã£o ruim
- MÃ£os fora do enquadramento
- Movimento muito rÃ¡pido

**SoluÃ§Ãµes**:
1. Melhorar iluminaÃ§Ã£o
2. Centralizar pessoa na cÃ¢mera
3. Fazer gestos mais devagar

---

## ğŸ“Š Estrutura Final

```
backend/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app.py                    # App Flask principal
â”‚   â”œâ”€â”€ config.py                 # ConfiguraÃ§Ãµes
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â””â”€â”€ gesture.py            # Rotas da API
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ mediapipe_handler.py  # Handler MediaPipe
â”‚       â””â”€â”€ model_loader.py       # Carregador do modelo
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_videos/               # VÃ­deos originais (180)
â”‚   â”œâ”€â”€ processed_videos/         # VÃ­deos sem fundo (180)
â”‚   â”œâ”€â”€ frames/                   # Frames extraÃ­dos (~5400)
â”‚   â”œâ”€â”€ keypoints/                # Keypoints JSON (180)
â”‚   â””â”€â”€ dataset/                  # Dataset preparado
â”‚       â”œâ”€â”€ X_train.npy
â”‚       â”œâ”€â”€ y_train.npy
â”‚       â”œâ”€â”€ X_val.npy
â”‚       â”œâ”€â”€ y_val.npy
â”‚       â”œâ”€â”€ X_test.npy
â”‚       â”œâ”€â”€ y_test.npy
â”‚       â”œâ”€â”€ gesture_mapping.json
â”‚       â””â”€â”€ dataset_info.json
â”œâ”€â”€ models/
â”‚   â””â”€â”€ trained/
â”‚       â”œâ”€â”€ best_gesture_model.h5        # â­ Modelo final
â”‚       â”œâ”€â”€ gesture_model_TIMESTAMP.h5
â”‚       â”œâ”€â”€ training_history_TIMESTAMP.png
â”‚       â””â”€â”€ training_history_TIMESTAMP.json
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ 1_remove_background.py
â”‚   â”œâ”€â”€ 2_extract_frames.py
â”‚   â”œâ”€â”€ 3_extract_keypoints.py
â”‚   â”œâ”€â”€ 4_prepare_dataset.py
â”‚   â””â”€â”€ 5_train_model.py
â”œâ”€â”€ venv/                         # Ambiente virtual
â”œâ”€â”€ requirements.txt              # DependÃªncias
â”œâ”€â”€ run_api.py                    # Script para rodar API
â””â”€â”€ README.md                     # Este arquivo
```

---

## ğŸ“ Autoria

**Projeto de QualificaÃ§Ã£o de Mestrado**
- Autor: Renato Wessner dos Santos
- InstituiÃ§Ã£o: UNIVESP
- Ano: 2025

---

## ğŸ“ LicenÃ§a

Propriedade reservada. Uso restrito ao projeto de mestrado.

---

## ğŸ†˜ Suporte

Para dÃºvidas ou problemas:
1. Verificar seÃ§Ã£o de Troubleshooting
2. Conferir logs do terminal
3. Validar estrutura de arquivos

---

**Feito com â¤ï¸ para a comunidade surda brasileira ğŸ¤Ÿ**
```

---

## ğŸ‰ **PROJETO COMPLETO!**

VocÃª agora tem:

âœ… Backend completo (5 scripts + API)  
âœ… README.md com documentaÃ§Ã£o detalhada  
âœ… Guia de gravaÃ§Ã£o profissional  
âœ… InstruÃ§Ãµes de integraÃ§Ã£o com frontend  
âœ… Troubleshooting completo  

**PrÃ³ximos passos:**
1. ğŸ“¹ Gravar vÃ­deos amanhÃ£ com iluminaÃ§Ã£o
2. ğŸ¬ Rodar scripts 1-5 em sequÃªncia
3. ğŸš€ Testar API
4. ğŸ”— Integrar com React

Alguma dÃºvida ou quer que eu adicione mais algo? ğŸ¤”